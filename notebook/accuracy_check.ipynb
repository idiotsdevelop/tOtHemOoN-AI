{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6fb574fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyupbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79055f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Lambda, Input, GlobalAveragePooling1D, Bidirectional\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77bb5439",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (64, 1) and (32, 1) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-7b77b7d32d77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mInput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../checkpoints/ckeckpointer.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2203\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2205\u001b[1;33m       \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2206\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2207\u001b[0m         raise NotImplementedError(\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1335\u001b[0m         options=options)\n\u001b[0;32m   1336\u001b[0m     base.CheckpointPosition(\n\u001b[1;32m-> 1337\u001b[1;33m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[0;32m   1338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m     \u001b[1;31m# Attached dependencies are not attached to the root, so should be restored\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m    971\u001b[0m     restore_ops.extend(\n\u001b[0;32m    972\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[1;32m--> 973\u001b[1;33m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[0;32m    974\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[0;32m    306\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[0;32m    307\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[1;32m--> 308\u001b[1;33m           validated_saveables).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[0;32m    309\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m       \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    319\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m           \u001b[0mrestore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    114\u001b[0m                                           structured_restored_tensors):\n\u001b[0;32m    115\u001b[0m       restore_ops[saveable.name] = saveable.restore(\n\u001b[1;32m--> 116\u001b[1;33m           restored_tensors, restored_shapes=None)\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    130\u001b[0m       \u001b[0mrestored_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[1;32m--> 132\u001b[1;33m           self.handle_op, self._var_shape, restored_tensor)\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mshape_safe_assign_variable_handle\u001b[1;34m(handle, shape, value, name)\u001b[0m\n\u001b[0;32m    305\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[0mvalue_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m   \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m   return gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m    309\u001b[0m       handle, value_tensor, name=name)\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \"\"\"\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (64, 1) and (32, 1) are incompatible"
     ]
    }
   ],
   "source": [
    "def get_model(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True, activation='tanh', dropout=0.2)(input)\n",
    "    x = LSTM(64, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "    x = LSTM(32, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    output = Dense(1)(x)\n",
    "    model = Model(input, output)\n",
    "    return model\n",
    "\n",
    "WINDOW_SIZE = 6\n",
    "feature_n = 6\n",
    "\n",
    "Input_shape = (WINDOW_SIZE, feature_n)\n",
    "model = get_model(Input_shape)\n",
    "model.load_weights('../checkpoints/ckeckpointer.ckpt')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c53e898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyupbit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "class Data_preprocess():\n",
    "    def __init__(self, args):\n",
    "        if not hasattr(args, 'data') or args.data is None:\n",
    "            print(args)\n",
    "            self.data, self.label, self.dataset = self.preprocess(\n",
    "                pyupbit.get_ohlcv(ticker=args.ticker, interval=args.interval, to=args.to, count=args.count))\n",
    "        else :\n",
    "            self.data, self.label, self.dataset = self.csv_parsing(args.data)\n",
    "\n",
    "    def MinMax(self, dataset_df):\n",
    "        norm = MinMaxScaler()\n",
    "        norm_dataset = norm.fit_transform(dataset_df)\n",
    "        return pd.DataFrame(norm_dataset, columns=list(dataset_df.columns))\n",
    "\n",
    "    def add_after10(self, dataset_df):\n",
    "        after10 = np.zeros_like(self.norm_dataset['close'])\n",
    "        for i in range(len(dataset_df['close']) - 1):\n",
    "            after10[i] = dataset_df['close'][i + 1]\n",
    "        return after10\n",
    "\n",
    "    def drop_feature(self, dataset_df):\n",
    "        # index(시간) 제거\n",
    "        dataset_df = dataset_df.reset_index(drop=True)\n",
    "        # value 제거\n",
    "        dataset_df = dataset_df.drop(columns=['value'])\n",
    "        return dataset_df\n",
    "\n",
    "    def add_avgPrice(self, dataset_df):\n",
    "        return (dataset_df['high'] + dataset_df['low'] +\n",
    "                dataset_df['open'] + dataset_df['close']) // 4\n",
    "\n",
    "    def preprocess(self, dataset, latest=False):\n",
    "\n",
    "        # drop feature\n",
    "        #dataset_df = self.drop_feature(dataset)\n",
    "        dataset_df = dataset\n",
    "\n",
    "        # avg_price 추가\n",
    "        dataset_df['avg_price'] = self.add_avgPrice(dataset_df)\n",
    "\n",
    "        if latest == True:\n",
    "            # 가장 예전 데이터 삭제 - norm이랑 original 둘 다 적용\n",
    "            self.dataset = self.dataset.drop([self.dataset.index[0]]).drop(columns=['after10'])\n",
    "            self.norm_dataset = self.norm_dataset.drop([self.norm_dataset.index[0]])\n",
    "\n",
    "            # ori dataset에 추가\n",
    "            self.dataset = pd.concat([self.dataset, dataset_df])\n",
    "            self.dataset = self.dataset.reset_index(drop=True)\n",
    "\n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(self.dataset)\n",
    "\n",
    "            # after10 추가\n",
    "            self.dataset['after10'] = self.add_after10(self.dataset)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(dataset_df)\n",
    "\n",
    "            # after10 추가\n",
    "            dataset_df['after10'] = self.add_after10(dataset_df)\n",
    "\n",
    "        # 예측될 값(label)인 10분 후 가격\n",
    "        self.norm_dataset['after10'] = self.add_after10(self.norm_dataset)\n",
    "\n",
    "        return self.norm_dataset.drop(columns=['after10']), self.norm_dataset['after10'], dataset_df\n",
    "\n",
    "    def csv_parsing(self, data_path):\n",
    "        merge_df = pd.DataFrame()\n",
    "        data_folders = glob(os.path.join(data_path, '*'))\n",
    "\n",
    "        for data_folder in tqdm(data_folders):\n",
    "            data_csvs = glob(os.path.join(data_folder,'*.csv'))\n",
    "\n",
    "            for data_csv in data_csvs :\n",
    "                csv_df = pd.read_csv(data_csv).drop(columns=[\"Unnamed: 0\"])\n",
    "                merge_df = pd.concat([merge_df, csv_df], ignore_index=True)\n",
    "\n",
    "        return self.preprocess(merge_df)\n",
    "\n",
    "\n",
    "    # dataset에 window 적용\n",
    "    def windowed_dataset(self, data, label, window_size, batch_size):\n",
    "        sliced_data = tf.data.Dataset.from_tensor_slices(data)\n",
    "        sliced_data = sliced_data.window(window_size, shift=1, stride=1, drop_remainder=True)\n",
    "        sliced_data = sliced_data.flat_map(lambda x: x.batch(window_size))\n",
    "\n",
    "        sliced_label = tf.data.Dataset.from_tensor_slices(label[window_size:])\n",
    "\n",
    "        sliced_dataset = tf.data.Dataset.zip((sliced_data, sliced_label))\n",
    "\n",
    "        return sliced_dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fdc92de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.backward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.backward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.backward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.backward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.backward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.backward_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 12, 7)]           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 12, 256)           139264    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 12, 128)           164352    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 12, 64)            41216     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 344,897\n",
      "Trainable params: 344,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model_BiLSTM(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, activation='tanh', dropout=0.3))(input)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, activation='tanh', dropout=0.3))(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, activation='tanh', dropout=0.3))(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    output = Dense(1)(x)\n",
    "    model = Model(input, output)\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True, activation='tanh', dropout=0.2)(input)\n",
    "    x = LSTM(64, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "    x = LSTM(32, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    output = Dense(1)(x)\n",
    "    model = Model(input, output)\n",
    "    return model\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 6\n",
    "feature_n = 6\n",
    "\n",
    "Bi_WINDOW_SIZE = 12\n",
    "Bi_feature_n = 7\n",
    "\n",
    "Input_shape = (Bi_WINDOW_SIZE, Bi_feature_n)\n",
    "model = get_model_BiLSTM(Input_shape)\n",
    "model.load_weights('./checkpoints/BiLSTM/ckeckpointer.ckpt')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9e7cb12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ticker': 'KRW-BTC', 'interval': 'minute10', 'to': '2021-11-16 09:48', 'count': 2000}\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "import datetime\n",
    "\n",
    "options = {\n",
    "    'ticker' : 'KRW-BTC',\n",
    "    'interval' : 'minute10',\n",
    "    'to' : datetime.datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "    'count' : 2000\n",
    "}\n",
    "\n",
    "args = EasyDict(options)\n",
    "\n",
    "processed_data = Data_preprocess(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c84e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          open      high       low     close    volume     value  avg_price\n",
      "0     0.067044  0.097650  0.075403  0.094142  0.186938  0.172054   0.082207\n",
      "1     0.094051  0.087999  0.071589  0.091501  0.160973  0.147976   0.085060\n",
      "2     0.095279  0.083604  0.079218  0.086973  0.146444  0.134660   0.084987\n",
      "3     0.086874  0.102236  0.092812  0.086690  0.293749  0.270809   0.090837\n",
      "4     0.092540  0.088859  0.057311  0.051222  0.153978  0.141464   0.071040\n",
      "...        ...       ...       ...       ...       ...       ...        ...\n",
      "1995  0.519358  0.512899  0.500636  0.478068  0.402771  0.392546   0.506514\n",
      "1996  0.478376  0.512803  0.485966  0.495236  0.438786  0.426934   0.496773\n",
      "1997  0.495562  0.492070  0.471394  0.448920  0.283932  0.275858   0.480457\n",
      "1998  0.448631  0.467132  0.460929  0.467975  0.306766  0.297327   0.464432\n",
      "1999  0.468366  0.464934  0.473252  0.461655  0.131033  0.127077   0.470354\n",
      "\n",
      "[2000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(processed_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b6cac5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.091501\n",
      "1       0.086973\n",
      "2       0.086690\n",
      "3       0.051222\n",
      "4       0.038487\n",
      "          ...   \n",
      "1995    0.495236\n",
      "1996    0.448920\n",
      "1997    0.467975\n",
      "1998    0.461655\n",
      "1999    0.000000\n",
      "Name: after10, Length: 2000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(processed_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8523026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(lstm_model, processed_data, window_size, batch_size):\n",
    "    dataset = processed_data.windowed_dataset(processed_data.data, processed_data.label, window_size, batch_size)\n",
    "    pred = lstm_model.predict(dataset)\n",
    "    pred = pred[:, 0]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41894a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bi_WINDOW_SIZE = 12\n",
    "\n",
    "WINDOW_SIZE = 6\n",
    "BATCH_SIZE=1\n",
    "\n",
    "pred = inference(model, processed_data, Bi_WINDOW_SIZE, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01efc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = processed_data.label[WINDOW_SIZE:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "281b90b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.017357\n",
       "1       0.005660\n",
       "2       0.020470\n",
       "3       0.009716\n",
       "4       0.000000\n",
       "          ...   \n",
       "1989    0.495236\n",
       "1990    0.448920\n",
       "1991    0.467975\n",
       "1992    0.461655\n",
       "1993    0.000000\n",
       "Name: after10, Length: 1994, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef29809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\quhb2\\anaconda3\\envs\\coin\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "pred_rate_val = []\n",
    "act_rate_val = []\n",
    "accuracy = []\n",
    "\n",
    "for i in range(0, len(pred) - 2) :\n",
    "    rate_pred = pred[i + 1]/pred[i]\n",
    "    pred_rate_val.append(rate_pred)\n",
    "    \n",
    "    if rate_pred < 1:\n",
    "        # Down\n",
    "        pred_tmp = 0\n",
    "    else:\n",
    "        # Up\n",
    "        pred_tmp = 1\n",
    "        \n",
    "    rate_act = actual[i + 2] / actual[i + 1]\n",
    "    act_rate_val.append(rate_act)\n",
    "    \n",
    "    if rate_act < 1:\n",
    "        # Down\n",
    "        act_tmp = 0\n",
    "    else:\n",
    "        # Up\n",
    "        act_tmp = 1\n",
    "        \n",
    "    if pred_tmp == act_tmp :\n",
    "        accuracy.append(1)\n",
    "    else :\n",
    "        accuracy.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "564770b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56797583081571"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도\n",
    "sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9126b024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_pred_rate_val = np.array(pred_rate_val)\n",
    "np_accuracy = np.array(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a095de42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0266085  1.0222495  1.0386864  1.0072212  1.0012504  1.002732\n",
      " 1.0104018  1.0259616  1.0147438  1.010896   1.0004454  1.0074378\n",
      " 0.9969732  0.9970859  0.99593    1.0015895  1.0062711  0.9989115\n",
      " 0.9928641  0.9923726  0.99101746 0.9839887  0.9988457  1.0043885\n",
      " 1.0098144  1.0138528  1.0195811  1.0270455  1.0192032  1.0209723\n",
      " 1.0058188  0.9971594  0.998505   0.992818   0.9875472  0.98794496\n",
      " 0.98872304 0.9849087  0.9958638  0.999887   0.9993528  0.9973917\n",
      " 0.9947604  1.0014883  1.0095834  1.002961   1.0003883  0.9994338\n",
      " 0.99592376 0.99026334 0.9884415  0.98716575 0.99206346 0.99592197\n",
      " 1.0043924  1.004332   1.0054196  1.002476   0.9959359  1.0008674\n",
      " 0.9976586  0.997627   1.0072576  1.0162153  1.0111749  1.002346\n",
      " 0.9974126  0.99525076 0.9896278  0.98797965 0.98788077 0.9803982\n",
      " 0.97631073 0.9712009  0.96584904 0.96451575 0.98727345 0.9935657\n",
      " 0.99218225 0.9967635  1.0579534  1.0326477  1.0121143  1.0094633\n",
      " 1.010294   1.0092967  1.0065305  1.0024788  1.002419   0.9940647\n",
      " 0.98060715 0.97456646 0.9716743  0.9711139  0.96957    0.9707679\n",
      " 0.9848354  1.0074908  0.9910149  0.9784793  0.98276687 0.9912382\n",
      " 0.99567765 0.999839   0.9997276  0.9955229  1.0061429  1.0043578\n",
      " 0.9986294  1.0131824  1.0123647  1.0084285  1.0058323  1.0002544\n",
      " 1.0009223  0.9933826  0.9882752  0.99576575 0.99637234 0.9945264\n",
      " 1.00346    1.0067519  1.0147371  1.0112929  1.015263   1.0172848\n",
      " 1.0141405  1.0142839  1.0198756  1.0217059  1.0234044  1.0213038\n",
      " 1.0194776  1.018441   1.012182   1.0027491  0.9966711  0.9902406\n",
      " 0.9799943  0.9744065  0.975797   0.99179465 0.9956737  0.99490696\n",
      " 0.999648   0.99985945 1.0064869  0.9995953  0.99388707 0.99371207\n",
      " 0.9999159  1.0061795  1.0035866  0.99347883 1.0077525  1.0162283\n",
      " 1.0113282  1.0037655  0.9959312  0.99949336 0.9997961  0.990546\n",
      " 0.9901119  0.98394996 0.97394705 0.9647481  0.96013665 0.9636763\n",
      " 0.96526253 0.9719179  0.96888083 0.98185223 0.971805   0.98507726\n",
      " 1.0255542  1.0384103  1.0266073  1.0126063  1.0097339  0.9955408\n",
      " 0.9898562  0.9942755  1.000054   0.9960432  0.99494594 0.99514765\n",
      " 0.9931042  1.000576   0.9998015  1.0064687  1.0191755  1.0158265\n",
      " 1.0082473  1.0028864  0.9998341  0.9977101  1.0006548  1.0062826\n",
      " 1.0078282  1.0089133  1.0050359  1.0022523  1.005074   1.0044837\n",
      " 1.0024722  1.0005187  1.0112518  1.0104505  1.0084252  1.0071597\n",
      " 1.0030668  1.0008417  0.9933461  0.98866796 0.99266315 0.9899036\n",
      " 0.9946438  0.998358   0.99783677 0.99853677 1.0009544  1.0009676\n",
      " 1.0045624  1.0026246  0.99273765 0.9839176  0.99002916 1.0033562\n",
      " 1.0211794  1.0343288  1.0446594  1.0606955  1.0629808  1.0252877\n",
      " 1.0126408  1.0191056  1.0229068  1.0162823  1.009474   1.004128\n",
      " 1.0032614  1.0014502  1.003967   1.0073899  1.009412   1.0030491\n",
      " 1.0037501  1.0063193  1.0033685  0.99895644 0.99668634 0.9980255\n",
      " 0.9986932  1.0002002  0.9963971  0.9934218  0.99288034 0.9868896\n",
      " 0.982163   0.9825135  0.9872675  0.99136907 0.9848253  0.98140043\n",
      " 0.97662985 0.96023244 0.9502987  0.9479392  0.9537838  0.969273\n",
      " 1.0116721  1.0302482  0.9853961  0.96430355 0.9590846  0.95998067\n",
      " 0.99585706 0.943258   0.9381194  1.0350009  0.9555416  0.92740524\n",
      " 0.9873187  1.0517534  1.0478451  1.0463595  1.0509135  1.0055952\n",
      " 0.9750575  0.96408623 0.9846272  0.97920656 0.9942221  0.97675127\n",
      " 0.967704   1.0026295  1.0031911  1.0316188  1.01365    1.006341\n",
      " 0.99278307 0.9924907  0.9845259  0.9978828  0.9970391  0.98152083\n",
      " 0.964497   0.97500163 0.9812153  0.99304634 1.0183212  1.0270437\n",
      " 1.0090388  1.0075265  0.9972179  0.9923379  0.9936384  0.99231374\n",
      " 0.99380416 0.9983782  1.006338   1.0150408  1.0110657  1.0076246\n",
      " 0.9993884  0.9979844  0.99472386 0.9865768  0.99248767 1.0076092\n",
      " 1.0065957  0.9966563  0.99701005 1.0069119  1.0100348  1.0089874\n",
      " 1.0268644  1.0272683  1.0185797  1.0059938  0.9923071  0.97739524\n",
      " 0.9789154  0.99091613 1.0054777  1.0245857  1.0447897  1.0278364\n",
      " 1.0128889  1.002966   1.0053933  0.99602    0.99438685 0.99869716\n",
      " 0.9991761  0.9992038  0.9929465  0.99327046 0.99769443 0.9965181\n",
      " 0.99577063 0.9964392  0.9938877  0.98476124 0.98238546 0.9935651\n",
      " 0.9853443  0.9887913  0.99590635 0.9907072  0.99983627 0.99851584\n",
      " 1.0019515  0.99502    0.9935146  0.9856985  0.9930779  1.0014542\n",
      " 1.006311   1.0167521  1.0327877  1.0147269  0.99977463 0.99153054\n",
      " 0.9807684  0.9813272  0.97627723 0.9856619  1.0017157  1.000914\n",
      " 1.0063442  1.0123936  1.0142428  1.0143992  1.0106256  1.005069\n",
      " 1.0012475  0.9961616  0.990636   0.99081427 0.9866135  0.9884336\n",
      " 0.99527514 0.9966745  0.99857277 1.0027882  1.0042847  1.001395\n",
      " 0.999963   0.9997878  0.99864966 1.0050273  1.0087564  1.0061387\n",
      " 1.0035875  0.9949757  0.99847674 0.99511063 0.9947605  0.99198276\n",
      " 0.9987213  1.0097522  1.0082421  1.0103434  1.017822   1.0173848\n",
      " 1.0163862  1.0169873  1.0148276  1.0020721  0.998126   0.991354\n",
      " 0.9847336  0.98000306 0.96990913 0.9689038  0.9664643  0.9712938\n",
      " 0.9798097  1.0343685  0.9971713  0.9819817  0.9696947  0.9733623\n",
      " 0.97578335 0.9887296  1.0461154  1.0649676  1.0372055  1.0145365\n",
      " 0.9935773  0.9845731  0.98520386 0.9686497  0.9787153  0.97750086\n",
      " 0.9795903  0.9835853  0.98935115 1.0023118  1.0259534  1.0075504\n",
      " 1.0059658  1.0079085  1.007997   1.0021714  0.9961918  0.994305\n",
      " 0.9970688  0.9942182  0.99370337 0.9959278  0.98856807 0.99219424\n",
      " 0.99865305 0.9992123  0.9940425  0.98441905 0.9762559  0.9677985\n",
      " 0.9836814  0.9934809  1.0169721  1.0110161  1.0153542  1.0213758\n",
      " 1.0262487  1.0223904  1.0100155  1.0027312  0.9894923  0.97342587\n",
      " 0.95100015 0.9329891  0.910341   0.902548   0.98887616 0.93626714\n",
      " 1.0040377  1.03917    1.0111802  0.99017864 1.0059166  1.0253823\n",
      " 1.0281942  1.0296118  1.0119686  0.9992808  0.99394727 0.9912512\n",
      " 0.9913291  1.0116452  0.9777589  0.9698585  0.98138255 0.99514747\n",
      " 1.0037044  1.0283376  1.0475199  1.0611429  1.0223256  1.0036645\n",
      " 0.98912096 0.9696279  0.96358234 0.960433   0.95049906 0.9431107\n",
      " 0.94636905 0.9676707  1.0107942  1.028345   0.99578696 0.96204853\n",
      " 0.94023293 0.96697    0.94219124 1.0145693  1.0329233  1.074599\n",
      " 1.0348891  1.0323153  1.0400261  1.0442348  1.0448452  1.0436465\n",
      " 1.0378256  1.0263194  1.0020045  0.99202245 0.9875237  1.0041603\n",
      " 1.0132105  1.0133423  1.0217831  1.032228   1.0244644  1.01466\n",
      " 1.0088315  1.0043566  0.9953904  0.9978517  1.0009795  1.0028026\n",
      " 1.0040898  1.0006124  0.99396473 0.9926213  1.002474   1.0023855\n",
      " 1.0011003  0.99586964 0.9952332  1.0034446  0.99902236 1.0067217\n",
      " 1.0145663  1.0092007  1.0100781  1.0139757  1.0110503  1.0205765\n",
      " 1.0173305  1.0269816  1.010442   0.9985915  0.99242973 0.9796202\n",
      " 0.97254294 0.9557329  0.94194216 0.94652015 0.9564968  0.97053325\n",
      " 0.9774509  0.99233156 1.005089   1.000678   0.9862311  0.9909593\n",
      " 1.001985   1.0032097  1.0085025  0.9941412  0.9905253  0.9853026\n",
      " 0.9825411  0.9806819  0.99274987 1.0077575  0.9972703  0.9848987\n",
      " 0.9803746  0.9843357  0.9870246  0.98206323 1.0038842  0.99134076\n",
      " 1.0055951  1.0024112  0.999634   1.0106642  1.0146372  1.025555\n",
      " 1.0221627  1.0254009  1.0300884  1.0305809  1.0241096  1.0086863\n",
      " 0.9959118  0.98217225 0.97612053 0.9752764  0.985114   0.9881749\n",
      " 0.97684085 0.97890824 0.9827425  0.98634315 0.9947628  0.99529356\n",
      " 1.0101116  1.008639   1.0219511  1.0251644  1.0212778  1.0157553\n",
      " 1.0051522  0.9917548  0.9896809  0.99098104 1.0012703  1.0024697\n",
      " 1.0054033  0.9973672  1.0018884  1.0069289  1.0177454  1.0055312\n",
      " 1.0138168  0.99902827 1.001736   1.0113739  1.0445774  1.0636575\n",
      " 1.0743487  1.0771011  1.0480065  1.0397966  1.0103877  1.0168853\n",
      " 0.9969309  0.9919643  0.9925971  1.0037183  0.9964373  1.0012136\n",
      " 1.0020564  0.996823   0.99261403 0.99234855 0.99142885 0.98173493\n",
      " 0.98674184 0.99605435 1.0037094  1.0000184  0.9864488  0.97855526\n",
      " 0.97291857 0.9622942  0.96094555 0.97600937 0.99588597 0.99392104\n",
      " 0.98104334 0.986015   0.99267256 0.9911742  0.99652594 1.0003333\n",
      " 1.010538   1.0165502  1.0132382  1.0216612  1.0146264  0.99842006\n",
      " 0.98222226 0.97592187 0.9848138  0.9918931  1.0043736  1.0288937\n",
      " 1.0159664  0.99931866 0.99827594 1.0066081  1.0016147  1.002379\n",
      " 1.0106777  1.009714   1.0097015  1.0293682  1.0440017  1.0470295\n",
      " 1.046544   1.0472195  1.0253752  1.0098866  0.99786735 0.9965478\n",
      " 0.9822121  0.98573816 0.98673123 0.9870455  1.0029631  1.0086956\n",
      " 1.0046716  0.9933508  0.99283177 1.002363   1.0048681  1.0080547\n",
      " 1.0084137  1.0033163  1.0033661  1.0008823  0.99925    1.001135\n",
      " 1.0104048  1.0112324  1.0125873  1.0073279  1.0034213  0.99809974\n",
      " 1.0053025  1.0089502  1.0080674  1.0160477  1.014467   1.0091561\n",
      " 1.0029098  0.9971664  1.0010389  0.99682254 1.0003704  1.0002571\n",
      " 0.9974353  0.99430865 0.9868278  0.98198783 0.9743841  0.97280586\n",
      " 0.9697222  0.9639015  0.9759983  0.9890835  0.9950828  0.9886208\n",
      " 0.98433024 0.9993063  0.99874926 1.0029691  1.0028765  1.012708\n",
      " 1.0087489  1.0076243  1.0013372  1.0013258  1.0022287  1.0042\n",
      " 0.99882156 0.999473   0.9981264  0.99126136 0.9936454  1.0024384\n",
      " 1.0075116  1.0089144  1.0064874  1.0137165  1.009551   1.0003064\n",
      " 0.9969629  1.000566   0.9999275  0.9957105  0.99613297 0.99284947\n",
      " 0.9860509  0.9906739  0.98280597 0.9801398  0.99307495 0.9836216\n",
      " 0.9726703  0.9716212  0.99723196 0.99523133 1.0029777  0.9953833\n",
      " 0.98415273 0.9860725  0.9822243  0.97413427 0.9787703  1.0092672\n",
      " 1.0140442  1.017497   1.0213213  1.0197849  1.0277485  1.0254511\n",
      " 1.0118898  0.99352133 1.0004898  1.006357   1.0069933  1.0060471\n",
      " 1.0156202  1.0168378  1.022226   1.019393   1.0116333  1.0129813\n",
      " 1.0106964  1.008334   1.0100154  1.0131865  1.0183035  1.0276995\n",
      " 1.0447898  1.0478885  1.0519432  1.0555408  1.0506815  1.0488942\n",
      " 1.0238669  1.0025887  0.9925335  0.97879684 0.9834526  0.9829826\n",
      " 0.99837106 1.0086297  1.0169706  1.0007253  0.9933222  0.99297833\n",
      " 0.9953953  0.9860148  0.98927486 1.0011903  1.0037252  1.000311\n",
      " 1.0038112  1.0067104  1.0041705  1.0072265  1.0076455  1.0033691\n",
      " 0.99923736 0.9961706  1.0107931  1.006832   1.0049767  1.0014535\n",
      " 0.9973505  0.9951848  0.9906587  0.9893196  0.9895285  0.9914209\n",
      " 0.9944037  0.9901003  0.985932   0.9869008  0.9868104  0.99424565\n",
      " 0.9963349  0.99506617 0.9975385  0.9946011  0.99866474 0.99380815\n",
      " 0.98599696 0.9957732  1.0081878  1.0232104  1.0303204  1.0214386\n",
      " 1.0145035  0.9973869  0.994516   0.9951158  0.99604636 0.99309975\n",
      " 0.9960998  0.9963281  1.004166   1.0015594  1.002815   1.0000596\n",
      " 0.9976215  0.9978091  0.99785924 0.99649405 0.9984401  1.0045372\n",
      " 1.0041215  1.007199   1.0031849  0.990333   0.986164   0.9769789\n",
      " 0.97668564 0.97640395 0.98220026 0.9968517  0.98663706 0.97840935\n",
      " 0.97256637 0.9742059  0.97345084 0.96054685 0.96206266 0.97540224\n",
      " 1.0041744  0.97736806 0.96073246 0.9626364  0.9769483  0.97430605\n",
      " 1.0013888  1.0074917  1.021021   1.0086564  1.014485   1.0113579\n",
      " 1.0061615  0.9963773  0.9874133  0.98171645 0.97696304 0.9837611\n",
      " 0.9824393  0.99260217 0.993469   0.9994917  1.0240855  1.0332388\n",
      " 1.022891   1.0120986  0.9996393  0.98732764 0.97643274 0.9854657\n",
      " 0.990705   0.99340606 0.99273205 0.988576   0.9898397  0.9760362\n",
      " 1.0164539  0.9828056  0.9726224  0.9727209  0.9748561  0.9799903\n",
      " 1.0049362  1.0425997 ]\n",
      "(992,)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "print(np_pred_rate_val)\n",
    "print(np_accuracy.shape)\n",
    "\n",
    "a_np_pred_rate_val =  copy.deepcopy(np_pred_rate_val)\n",
    "b_np_pred_rate_val =  copy.deepcopy(np_pred_rate_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "498f97a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.0222495 , 0.        , 1.0072212 , 1.0012504 ,\n",
       "       0.        , 1.0104018 , 1.0259616 , 0.        , 0.        ,\n",
       "       1.0004454 , 0.        , 0.9969732 , 0.        , 0.99593   ,\n",
       "       0.        , 1.0062711 , 0.        , 0.9928641 , 0.9923726 ,\n",
       "       0.99101746, 0.9839887 , 0.        , 1.0043885 , 0.        ,\n",
       "       1.0138528 , 0.        , 1.0270455 , 1.0192032 , 1.0209723 ,\n",
       "       1.0058188 , 0.9971594 , 0.998505  , 0.        , 0.        ,\n",
       "       0.98794496, 0.        , 0.9849087 , 0.        , 0.999887  ,\n",
       "       0.9993528 , 0.9973917 , 0.        , 0.        , 1.0095834 ,\n",
       "       0.        , 1.0003883 , 0.9994338 , 0.        , 0.99026334,\n",
       "       0.        , 0.98716575, 0.        , 0.        , 1.0043924 ,\n",
       "       0.        , 0.        , 1.002476  , 0.        , 0.        ,\n",
       "       0.        , 0.997627  , 1.0072576 , 1.0162153 , 1.0111749 ,\n",
       "       1.002346  , 0.        , 0.        , 0.        , 0.98797965,\n",
       "       0.        , 0.        , 0.97631073, 0.9712009 , 0.96584904,\n",
       "       0.96451575, 0.98727345, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0121143 , 0.        , 1.010294  ,\n",
       "       1.0092967 , 1.0065305 , 1.0024788 , 1.002419  , 0.9940647 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0074908 , 0.9910149 , 0.        ,\n",
       "       0.        , 0.9912382 , 0.        , 0.999839  , 0.9997276 ,\n",
       "       0.9955229 , 0.        , 1.0043578 , 0.        , 0.        ,\n",
       "       0.        , 1.0084285 , 1.0058323 , 1.0002544 , 0.        ,\n",
       "       0.        , 0.9882752 , 0.99576575, 0.99637234, 0.        ,\n",
       "       0.        , 1.0067519 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.0234044 , 1.0213038 , 1.0194776 , 0.        , 1.012182  ,\n",
       "       1.0027491 , 0.        , 0.9902406 , 0.9799943 , 0.        ,\n",
       "       0.        , 0.99179465, 0.9956737 , 0.99490696, 0.        ,\n",
       "       0.        , 1.0064869 , 0.9995953 , 0.99388707, 0.        ,\n",
       "       0.9999159 , 0.        , 1.0035866 , 0.        , 0.        ,\n",
       "       1.0162283 , 0.        , 1.0037655 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.98394996, 0.        ,\n",
       "       0.        , 0.        , 0.9636763 , 0.96526253, 0.9719179 ,\n",
       "       0.96888083, 0.98185223, 0.        , 0.98507726, 0.        ,\n",
       "       1.0384103 , 1.0266073 , 0.        , 0.        , 0.9955408 ,\n",
       "       0.        , 0.        , 1.000054  , 0.9960432 , 0.99494594,\n",
       "       0.99514765, 0.        , 1.000576  , 0.        , 1.0064687 ,\n",
       "       0.        , 0.        , 1.0082473 , 1.0028864 , 0.9998341 ,\n",
       "       0.9977101 , 1.0006548 , 0.        , 0.        , 1.0089133 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.0024722 ,\n",
       "       0.        , 1.0112518 , 1.0104505 , 0.        , 1.0071597 ,\n",
       "       0.        , 1.0008417 , 0.9933461 , 0.        , 0.99266315,\n",
       "       0.9899036 , 0.9946438 , 0.        , 0.99783677, 0.        ,\n",
       "       1.0009544 , 1.0009676 , 1.0045624 , 0.        , 0.99273765,\n",
       "       0.9839176 , 0.99002916, 0.        , 1.0211794 , 1.0343288 ,\n",
       "       0.        , 0.        , 1.0629808 , 1.0252877 , 0.        ,\n",
       "       0.        , 0.        , 1.0162823 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.0073899 , 0.        ,\n",
       "       1.0030491 , 1.0037501 , 0.        , 1.0033685 , 0.99895644,\n",
       "       0.        , 0.        , 0.9986932 , 1.0002002 , 0.        ,\n",
       "       0.        , 0.99288034, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.9848253 , 0.98140043, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9537838 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.9590846 ,\n",
       "       0.        , 0.        , 0.943258  , 0.9381194 , 0.        ,\n",
       "       0.        , 0.        , 0.9873187 , 1.0517534 , 0.        ,\n",
       "       0.        , 1.0509135 , 1.0055952 , 0.9750575 , 0.        ,\n",
       "       0.9846272 , 0.97920656, 0.9942221 , 0.        , 0.        ,\n",
       "       1.0026295 , 0.        , 0.        , 1.01365   , 0.        ,\n",
       "       0.        , 0.        , 0.9845259 , 0.9978828 , 0.9970391 ,\n",
       "       0.98152083, 0.964497  , 0.        , 0.9812153 , 0.99304634,\n",
       "       1.0183212 , 1.0270437 , 0.        , 0.        , 0.9972179 ,\n",
       "       0.9923379 , 0.9936384 , 0.99231374, 0.99380416, 0.        ,\n",
       "       1.006338  , 1.0150408 , 0.        , 1.0076246 , 0.9993884 ,\n",
       "       0.9979844 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9966563 , 0.99701005, 1.0069119 , 1.0100348 ,\n",
       "       0.        , 1.0268644 , 1.0272683 , 0.        , 0.        ,\n",
       "       0.        , 0.97739524, 0.9789154 , 0.        , 1.0054777 ,\n",
       "       0.        , 1.0447897 , 1.0278364 , 1.0128889 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.9991761 ,\n",
       "       0.        , 0.        , 0.99327046, 0.        , 0.9965181 ,\n",
       "       0.        , 0.        , 0.        , 0.98476124, 0.        ,\n",
       "       0.9935651 , 0.9853443 , 0.        , 0.99590635, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.99502   , 0.9935146 ,\n",
       "       0.        , 0.9930779 , 1.0014542 , 1.006311  , 1.0167521 ,\n",
       "       1.0327877 , 1.0147269 , 0.        , 0.        , 0.9807684 ,\n",
       "       0.9813272 , 0.97627723, 0.        , 0.        , 1.000914  ,\n",
       "       0.        , 1.0123936 , 1.0142428 , 1.0143992 , 0.        ,\n",
       "       1.005069  , 1.0012475 , 0.9961616 , 0.990636  , 0.        ,\n",
       "       0.9866135 , 0.        , 0.        , 0.        , 0.99857277,\n",
       "       0.        , 0.        , 1.001395  , 0.999963  , 0.        ,\n",
       "       0.99864966, 1.0050273 , 1.0087564 , 0.        , 0.        ,\n",
       "       0.        , 0.99847674, 0.99511063, 0.9947605 , 0.99198276,\n",
       "       0.9987213 , 1.0097522 , 0.        , 0.        , 0.        ,\n",
       "       1.0173848 , 1.0163862 , 1.0169873 , 1.0148276 , 1.0020721 ,\n",
       "       0.        , 0.        , 0.9847336 , 0.        , 0.96990913,\n",
       "       0.        , 0.        , 0.        , 0.9798097 , 0.        ,\n",
       "       0.9971713 , 0.9819817 , 0.        , 0.9733623 , 0.97578335,\n",
       "       0.        , 0.        , 1.0649676 , 1.0372055 , 0.        ,\n",
       "       0.        , 0.9845731 , 0.98520386, 0.        , 0.        ,\n",
       "       0.97750086, 0.9795903 , 0.9835853 , 0.98935115, 1.0023118 ,\n",
       "       0.        , 1.0075504 , 0.        , 0.        , 1.007997  ,\n",
       "       1.0021714 , 0.        , 0.        , 0.9970688 , 0.9942182 ,\n",
       "       0.99370337, 0.        , 0.        , 0.        , 0.99865305,\n",
       "       0.        , 0.        , 0.98441905, 0.9762559 , 0.9677985 ,\n",
       "       0.9836814 , 0.9934809 , 0.        , 1.0110161 , 1.0153542 ,\n",
       "       1.0213758 , 1.0262487 , 1.0223904 , 1.0100155 , 1.0027312 ,\n",
       "       0.        , 0.97342587, 0.95100015, 0.        , 0.910341  ,\n",
       "       0.        , 0.        , 0.93626714, 1.0040377 , 1.03917   ,\n",
       "       1.0111802 , 0.        , 0.        , 0.        , 1.0281942 ,\n",
       "       1.0296118 , 1.0119686 , 0.9992808 , 0.        , 0.        ,\n",
       "       0.9913291 , 0.        , 0.9777589 , 0.9698585 , 0.98138255,\n",
       "       0.        , 1.0037044 , 0.        , 0.        , 1.0611429 ,\n",
       "       1.0223256 , 1.0036645 , 0.98912096, 0.9696279 , 0.96358234,\n",
       "       0.        , 0.95049906, 0.9431107 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.94023293,\n",
       "       0.        , 0.94219124, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0400261 , 0.        , 1.0448452 ,\n",
       "       0.        , 1.0378256 , 0.        , 1.0020045 , 0.99202245,\n",
       "       0.9875237 , 0.        , 1.0132105 , 0.        , 1.0217831 ,\n",
       "       1.032228  , 0.        , 0.        , 1.0088315 , 0.        ,\n",
       "       0.        , 0.9978517 , 0.        , 1.0028026 , 0.        ,\n",
       "       1.0006124 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.99902236,\n",
       "       0.        , 1.0145663 , 0.        , 0.        , 1.0139757 ,\n",
       "       0.        , 1.0205765 , 1.0173305 , 1.0269816 , 1.010442  ,\n",
       "       0.        , 0.        , 0.9796202 , 0.97254294, 0.9557329 ,\n",
       "       0.        , 0.94652015, 0.        , 0.97053325, 0.9774509 ,\n",
       "       0.        , 0.        , 1.000678  , 0.9862311 , 0.        ,\n",
       "       1.001985  , 1.0032097 , 0.        , 0.        , 0.        ,\n",
       "       0.9853026 , 0.        , 0.        , 0.99274987, 1.0077575 ,\n",
       "       0.9972703 , 0.        , 0.        , 0.9843357 , 0.        ,\n",
       "       0.98206323, 1.0038842 , 0.99134076, 0.        , 0.        ,\n",
       "       0.999634  , 0.        , 1.0146372 , 0.        , 1.0221627 ,\n",
       "       1.0254009 , 1.0300884 , 0.        , 1.0241096 , 1.0086863 ,\n",
       "       0.9959118 , 0.        , 0.97612053, 0.        , 0.985114  ,\n",
       "       0.9881749 , 0.        , 0.97890824, 0.9827425 , 0.98634315,\n",
       "       0.9947628 , 0.99529356, 1.0101116 , 1.008639  , 0.        ,\n",
       "       1.0251644 , 0.        , 1.0157553 , 1.0051522 , 0.9917548 ,\n",
       "       0.9896809 , 0.99098104, 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.0018884 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.001736  , 0.        , 1.0445774 ,\n",
       "       0.        , 1.0743487 , 1.0771011 , 1.0480065 , 0.        ,\n",
       "       0.        , 1.0168853 , 0.        , 0.9919643 , 0.9925971 ,\n",
       "       1.0037183 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.99234855, 0.99142885, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.97855526,\n",
       "       0.97291857, 0.        , 0.        , 0.        , 0.99588597,\n",
       "       0.99392104, 0.        , 0.986015  , 0.99267256, 0.9911742 ,\n",
       "       0.        , 1.0003333 , 1.010538  , 0.        , 1.0132382 ,\n",
       "       0.        , 1.0146264 , 0.        , 0.98222226, 0.97592187,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.0159664 ,\n",
       "       0.        , 0.99827594, 1.0066081 , 0.        , 0.        ,\n",
       "       0.        , 1.009714  , 0.        , 1.0293682 , 0.        ,\n",
       "       1.0470295 , 1.046544  , 1.0472195 , 1.0253752 , 0.        ,\n",
       "       0.99786735, 0.        , 0.        , 0.        , 0.98673123,\n",
       "       0.        , 0.        , 1.0086956 , 0.        , 0.9933508 ,\n",
       "       0.        , 1.002363  , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0008823 , 0.99925   , 0.        ,\n",
       "       1.0104048 , 0.        , 1.0125873 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0089502 , 0.        , 1.0160477 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.0003704 , 1.0002571 , 0.        , 0.        ,\n",
       "       0.9868278 , 0.        , 0.        , 0.97280586, 0.        ,\n",
       "       0.        , 0.        , 0.9890835 , 0.9950828 , 0.9886208 ,\n",
       "       0.        , 0.9993063 , 0.99874926, 1.0029691 , 0.        ,\n",
       "       0.        , 1.0087489 , 1.0076243 , 1.0013372 , 0.        ,\n",
       "       0.        , 1.0042    , 0.        , 0.999473  , 0.9981264 ,\n",
       "       0.        , 0.9936454 , 1.0024384 , 0.        , 1.0089144 ,\n",
       "       0.        , 0.        , 1.009551  , 1.0003064 , 0.        ,\n",
       "       1.000566  , 0.9999275 , 0.9957105 , 0.        , 0.        ,\n",
       "       0.9860509 , 0.        , 0.98280597, 0.        , 0.        ,\n",
       "       0.9836216 , 0.9726703 , 0.        , 0.        , 0.99523133,\n",
       "       0.        , 0.9953833 , 0.98415273, 0.        , 0.9822243 ,\n",
       "       0.97413427, 0.9787703 , 1.0092672 , 0.        , 0.        ,\n",
       "       0.        , 1.0197849 , 0.        , 1.0254511 , 0.        ,\n",
       "       0.        , 1.0004898 , 0.        , 0.        , 1.0060471 ,\n",
       "       0.        , 1.0168378 , 1.022226  , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0447898 , 0.        , 1.0519432 ,\n",
       "       1.0555408 , 1.0506815 , 0.        , 0.        , 1.0025887 ,\n",
       "       0.9925335 , 0.97879684, 0.9834526 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0007253 , 0.9933222 , 0.99297833,\n",
       "       0.        , 0.9860148 , 0.98927486, 1.0011903 , 0.        ,\n",
       "       0.        , 0.        , 1.0067104 , 0.        , 1.0072265 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.0107931 ,\n",
       "       1.006832  , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.9895285 , 0.        , 0.        ,\n",
       "       0.        , 0.985932  , 0.9869008 , 0.        , 0.        ,\n",
       "       0.        , 0.99506617, 0.        , 0.9946011 , 0.99866474,\n",
       "       0.        , 0.        , 0.9957732 , 0.        , 0.        ,\n",
       "       1.0303204 , 1.0214386 , 0.        , 0.        , 0.994516  ,\n",
       "       0.        , 0.        , 0.99309975, 0.9960998 , 0.        ,\n",
       "       1.004166  , 1.0015594 , 1.002815  , 0.        , 0.9976215 ,\n",
       "       0.9978091 , 0.        , 0.99649405, 0.        , 1.0045372 ,\n",
       "       1.0041215 , 1.007199  , 1.0031849 , 0.        , 0.        ,\n",
       "       0.9769789 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.98663706, 0.        , 0.        , 0.9742059 , 0.        ,\n",
       "       0.96054685, 0.        , 0.97540224, 0.        , 0.        ,\n",
       "       0.96073246, 0.9626364 , 0.9769483 , 0.        , 0.        ,\n",
       "       1.0074917 , 1.021021  , 0.        , 1.014485  , 0.        ,\n",
       "       1.0061615 , 0.        , 0.9874133 , 0.        , 0.97696304,\n",
       "       0.9837611 , 0.9824393 , 0.99260217, 0.        , 0.        ,\n",
       "       1.0240855 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.97643274, 0.        , 0.        , 0.        ,\n",
       "       0.99273205, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.9828056 , 0.9726224 , 0.        , 0.9748561 , 0.        ,\n",
       "       0.        , 1.0425997 ], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_np_pred_rate_val[ np_accuracy[:]==1 ] = 0\n",
    "a_np_pred_rate_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d40d972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0266085 , 0.        , 1.0386864 , 0.        , 0.        ,\n",
       "       1.002732  , 0.        , 0.        , 1.0147438 , 1.010896  ,\n",
       "       0.        , 1.0074378 , 0.        , 0.9970859 , 0.        ,\n",
       "       1.0015895 , 0.        , 0.9989115 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.9988457 , 0.        , 1.0098144 ,\n",
       "       0.        , 1.0195811 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.992818  , 0.9875472 ,\n",
       "       0.        , 0.98872304, 0.        , 0.9958638 , 0.        ,\n",
       "       0.        , 0.        , 0.9947604 , 1.0014883 , 0.        ,\n",
       "       1.002961  , 0.        , 0.        , 0.99592376, 0.        ,\n",
       "       0.9884415 , 0.        , 0.99206346, 0.99592197, 0.        ,\n",
       "       1.004332  , 1.0054196 , 0.        , 0.9959359 , 1.0008674 ,\n",
       "       0.9976586 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9974126 , 0.99525076, 0.9896278 , 0.        ,\n",
       "       0.98788077, 0.9803982 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.9935657 , 0.99218225, 0.9967635 ,\n",
       "       1.0579534 , 1.0326477 , 0.        , 1.0094633 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.98060715, 0.97456646, 0.9716743 , 0.9711139 , 0.96957   ,\n",
       "       0.9707679 , 0.9848354 , 0.        , 0.        , 0.9784793 ,\n",
       "       0.98276687, 0.        , 0.99567765, 0.        , 0.        ,\n",
       "       0.        , 1.0061429 , 0.        , 0.9986294 , 1.0131824 ,\n",
       "       1.0123647 , 0.        , 0.        , 0.        , 1.0009223 ,\n",
       "       0.9933826 , 0.        , 0.        , 0.        , 0.9945264 ,\n",
       "       1.00346   , 0.        , 1.0147371 , 1.0112929 , 1.015263  ,\n",
       "       1.0172848 , 1.0141405 , 1.0142839 , 1.0198756 , 1.0217059 ,\n",
       "       0.        , 0.        , 0.        , 1.018441  , 0.        ,\n",
       "       0.        , 0.9966711 , 0.        , 0.        , 0.9744065 ,\n",
       "       0.975797  , 0.        , 0.        , 0.        , 0.999648  ,\n",
       "       0.99985945, 0.        , 0.        , 0.        , 0.99371207,\n",
       "       0.        , 1.0061795 , 0.        , 0.99347883, 1.0077525 ,\n",
       "       0.        , 1.0113282 , 0.        , 0.9959312 , 0.99949336,\n",
       "       0.9997961 , 0.990546  , 0.9901119 , 0.        , 0.97394705,\n",
       "       0.9647481 , 0.96013665, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.971805  , 0.        , 1.0255542 ,\n",
       "       0.        , 0.        , 1.0126063 , 1.0097339 , 0.        ,\n",
       "       0.9898562 , 0.9942755 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9931042 , 0.        , 0.9998015 , 0.        ,\n",
       "       1.0191755 , 1.0158265 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0062826 , 1.0078282 , 0.        ,\n",
       "       1.0050359 , 1.0022523 , 1.005074  , 1.0044837 , 0.        ,\n",
       "       1.0005187 , 0.        , 0.        , 1.0084252 , 0.        ,\n",
       "       1.0030668 , 0.        , 0.        , 0.98866796, 0.        ,\n",
       "       0.        , 0.        , 0.998358  , 0.        , 0.99853677,\n",
       "       0.        , 0.        , 0.        , 1.0026246 , 0.        ,\n",
       "       0.        , 0.        , 1.0033562 , 0.        , 0.        ,\n",
       "       1.0446594 , 1.0606955 , 0.        , 0.        , 1.0126408 ,\n",
       "       1.0191056 , 1.0229068 , 0.        , 1.009474  , 1.004128  ,\n",
       "       1.0032614 , 1.0014502 , 1.003967  , 0.        , 1.009412  ,\n",
       "       0.        , 0.        , 1.0063193 , 0.        , 0.        ,\n",
       "       0.99668634, 0.9980255 , 0.        , 0.        , 0.9963971 ,\n",
       "       0.9934218 , 0.        , 0.9868896 , 0.982163  , 0.9825135 ,\n",
       "       0.9872675 , 0.99136907, 0.        , 0.        , 0.97662985,\n",
       "       0.96023244, 0.9502987 , 0.9479392 , 0.        , 0.969273  ,\n",
       "       1.0116721 , 1.0302482 , 0.9853961 , 0.96430355, 0.        ,\n",
       "       0.95998067, 0.99585706, 0.        , 0.        , 1.0350009 ,\n",
       "       0.9555416 , 0.92740524, 0.        , 0.        , 1.0478451 ,\n",
       "       1.0463595 , 0.        , 0.        , 0.        , 0.96408623,\n",
       "       0.        , 0.        , 0.        , 0.97675127, 0.967704  ,\n",
       "       0.        , 1.0031911 , 1.0316188 , 0.        , 1.006341  ,\n",
       "       0.99278307, 0.9924907 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.97500163, 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0090388 , 1.0075265 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.9983782 ,\n",
       "       0.        , 0.        , 1.0110657 , 0.        , 0.        ,\n",
       "       0.        , 0.99472386, 0.9865768 , 0.99248767, 1.0076092 ,\n",
       "       1.0065957 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.0089874 , 0.        , 0.        , 1.0185797 , 1.0059938 ,\n",
       "       0.9923071 , 0.        , 0.        , 0.99091613, 0.        ,\n",
       "       1.0245857 , 0.        , 0.        , 0.        , 1.002966  ,\n",
       "       1.0053933 , 0.99602   , 0.99438685, 0.99869716, 0.        ,\n",
       "       0.9992038 , 0.9929465 , 0.        , 0.99769443, 0.        ,\n",
       "       0.99577063, 0.9964392 , 0.9938877 , 0.        , 0.98238546,\n",
       "       0.        , 0.        , 0.9887913 , 0.        , 0.9907072 ,\n",
       "       0.99983627, 0.99851584, 1.0019515 , 0.        , 0.        ,\n",
       "       0.9856985 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.99977463, 0.99153054, 0.        ,\n",
       "       0.        , 0.        , 0.9856619 , 1.0017157 , 0.        ,\n",
       "       1.0063442 , 0.        , 0.        , 0.        , 1.0106256 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.99081427,\n",
       "       0.        , 0.9884336 , 0.99527514, 0.9966745 , 0.        ,\n",
       "       1.0027882 , 1.0042847 , 0.        , 0.        , 0.9997878 ,\n",
       "       0.        , 0.        , 0.        , 1.0061387 , 1.0035875 ,\n",
       "       0.9949757 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0082421 , 1.0103434 , 1.017822  ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.998126  , 0.991354  , 0.        , 0.98000306, 0.        ,\n",
       "       0.9689038 , 0.9664643 , 0.9712938 , 0.        , 1.0343685 ,\n",
       "       0.        , 0.        , 0.9696947 , 0.        , 0.        ,\n",
       "       0.9887296 , 1.0461154 , 0.        , 0.        , 1.0145365 ,\n",
       "       0.9935773 , 0.        , 0.        , 0.9686497 , 0.9787153 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.0259534 , 0.        , 1.0059658 , 1.0079085 , 0.        ,\n",
       "       0.        , 0.9961918 , 0.994305  , 0.        , 0.        ,\n",
       "       0.        , 0.9959278 , 0.98856807, 0.99219424, 0.        ,\n",
       "       0.9992123 , 0.9940425 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0169721 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.9894923 , 0.        , 0.        , 0.9329891 , 0.        ,\n",
       "       0.902548  , 0.98887616, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.99017864, 1.0059166 , 1.0253823 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.99394727, 0.9912512 ,\n",
       "       0.        , 1.0116452 , 0.        , 0.        , 0.        ,\n",
       "       0.99514747, 0.        , 1.0283376 , 1.0475199 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.960433  , 0.        , 0.        , 0.94636905, 0.9676707 ,\n",
       "       1.0107942 , 1.028345  , 0.99578696, 0.96204853, 0.        ,\n",
       "       0.96697   , 0.        , 1.0145693 , 1.0329233 , 1.074599  ,\n",
       "       1.0348891 , 1.0323153 , 0.        , 1.0442348 , 0.        ,\n",
       "       1.0436465 , 0.        , 1.0263194 , 0.        , 0.        ,\n",
       "       0.        , 1.0041603 , 0.        , 1.0133423 , 0.        ,\n",
       "       0.        , 1.0244644 , 1.01466   , 0.        , 1.0043566 ,\n",
       "       0.9953904 , 0.        , 1.0009795 , 0.        , 1.0040898 ,\n",
       "       0.        , 0.99396473, 0.9926213 , 1.002474  , 1.0023855 ,\n",
       "       1.0011003 , 0.99586964, 0.9952332 , 1.0034446 , 0.        ,\n",
       "       1.0067217 , 0.        , 1.0092007 , 1.0100781 , 0.        ,\n",
       "       1.0110503 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.9985915 , 0.99242973, 0.        , 0.        , 0.        ,\n",
       "       0.94194216, 0.        , 0.9564968 , 0.        , 0.        ,\n",
       "       0.99233156, 1.005089  , 0.        , 0.        , 0.9909593 ,\n",
       "       0.        , 0.        , 1.0085025 , 0.9941412 , 0.9905253 ,\n",
       "       0.        , 0.9825411 , 0.9806819 , 0.        , 0.        ,\n",
       "       0.        , 0.9848987 , 0.9803746 , 0.        , 0.9870246 ,\n",
       "       0.        , 0.        , 0.        , 1.0055951 , 1.0024112 ,\n",
       "       0.        , 1.0106642 , 0.        , 1.025555  , 0.        ,\n",
       "       0.        , 0.        , 1.0305809 , 0.        , 0.        ,\n",
       "       0.        , 0.98217225, 0.        , 0.9752764 , 0.        ,\n",
       "       0.        , 0.97684085, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.0219511 ,\n",
       "       0.        , 1.0212778 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.0012703 , 1.0024697 , 1.0054033 ,\n",
       "       0.9973672 , 0.        , 1.0069289 , 1.0177454 , 1.0055312 ,\n",
       "       1.0138168 , 0.99902827, 0.        , 1.0113739 , 0.        ,\n",
       "       1.0636575 , 0.        , 0.        , 0.        , 1.0397966 ,\n",
       "       1.0103877 , 0.        , 0.9969309 , 0.        , 0.        ,\n",
       "       0.        , 0.9964373 , 1.0012136 , 1.0020564 , 0.996823  ,\n",
       "       0.99261403, 0.        , 0.        , 0.98173493, 0.98674184,\n",
       "       0.99605435, 1.0037094 , 1.0000184 , 0.9864488 , 0.        ,\n",
       "       0.        , 0.9622942 , 0.96094555, 0.97600937, 0.        ,\n",
       "       0.        , 0.98104334, 0.        , 0.        , 0.        ,\n",
       "       0.99652594, 0.        , 0.        , 1.0165502 , 0.        ,\n",
       "       1.0216612 , 0.        , 0.99842006, 0.        , 0.        ,\n",
       "       0.9848138 , 0.9918931 , 1.0043736 , 1.0288937 , 0.        ,\n",
       "       0.99931866, 0.        , 0.        , 1.0016147 , 1.002379  ,\n",
       "       1.0106777 , 0.        , 1.0097015 , 0.        , 1.0440017 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.0098866 ,\n",
       "       0.        , 0.9965478 , 0.9822121 , 0.98573816, 0.        ,\n",
       "       0.9870455 , 1.0029631 , 0.        , 1.0046716 , 0.        ,\n",
       "       0.99283177, 0.        , 1.0048681 , 1.0080547 , 1.0084137 ,\n",
       "       1.0033163 , 1.0033661 , 0.        , 0.        , 1.001135  ,\n",
       "       0.        , 1.0112324 , 0.        , 1.0073279 , 1.0034213 ,\n",
       "       0.99809974, 1.0053025 , 0.        , 1.0080674 , 0.        ,\n",
       "       1.014467  , 1.0091561 , 1.0029098 , 0.9971664 , 1.0010389 ,\n",
       "       0.99682254, 0.        , 0.        , 0.9974353 , 0.99430865,\n",
       "       0.        , 0.98198783, 0.9743841 , 0.        , 0.9697222 ,\n",
       "       0.9639015 , 0.9759983 , 0.        , 0.        , 0.        ,\n",
       "       0.98433024, 0.        , 0.        , 0.        , 1.0028765 ,\n",
       "       1.012708  , 0.        , 0.        , 0.        , 1.0013258 ,\n",
       "       1.0022287 , 0.        , 0.99882156, 0.        , 0.        ,\n",
       "       0.99126136, 0.        , 0.        , 1.0075116 , 0.        ,\n",
       "       1.0064874 , 1.0137165 , 0.        , 0.        , 0.9969629 ,\n",
       "       0.        , 0.        , 0.        , 0.99613297, 0.99284947,\n",
       "       0.        , 0.9906739 , 0.        , 0.9801398 , 0.99307495,\n",
       "       0.        , 0.        , 0.9716212 , 0.99723196, 0.        ,\n",
       "       1.0029777 , 0.        , 0.        , 0.9860725 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.0140442 , 1.017497  ,\n",
       "       1.0213213 , 0.        , 1.0277485 , 0.        , 1.0118898 ,\n",
       "       0.99352133, 0.        , 1.006357  , 1.0069933 , 0.        ,\n",
       "       1.0156202 , 0.        , 0.        , 1.019393  , 1.0116333 ,\n",
       "       1.0129813 , 1.0106964 , 1.008334  , 1.0100154 , 1.0131865 ,\n",
       "       1.0183035 , 1.0276995 , 0.        , 1.0478885 , 0.        ,\n",
       "       0.        , 0.        , 1.0488942 , 1.0238669 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9829826 , 0.99837106,\n",
       "       1.0086297 , 1.0169706 , 0.        , 0.        , 0.        ,\n",
       "       0.9953953 , 0.        , 0.        , 0.        , 1.0037252 ,\n",
       "       1.000311  , 1.0038112 , 0.        , 1.0041705 , 0.        ,\n",
       "       1.0076455 , 1.0033691 , 0.99923736, 0.9961706 , 0.        ,\n",
       "       0.        , 1.0049767 , 1.0014535 , 0.9973505 , 0.9951848 ,\n",
       "       0.9906587 , 0.9893196 , 0.        , 0.9914209 , 0.9944037 ,\n",
       "       0.9901003 , 0.        , 0.        , 0.9868104 , 0.99424565,\n",
       "       0.9963349 , 0.        , 0.9975385 , 0.        , 0.        ,\n",
       "       0.99380815, 0.98599696, 0.        , 1.0081878 , 1.0232104 ,\n",
       "       0.        , 0.        , 1.0145035 , 0.9973869 , 0.        ,\n",
       "       0.9951158 , 0.99604636, 0.        , 0.        , 0.9963281 ,\n",
       "       0.        , 0.        , 0.        , 1.0000596 , 0.        ,\n",
       "       0.        , 0.99785924, 0.        , 0.9984401 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.990333  , 0.986164  ,\n",
       "       0.        , 0.97668564, 0.97640395, 0.98220026, 0.9968517 ,\n",
       "       0.        , 0.97840935, 0.97256637, 0.        , 0.97345084,\n",
       "       0.        , 0.96206266, 0.        , 1.0041744 , 0.97736806,\n",
       "       0.        , 0.        , 0.        , 0.97430605, 1.0013888 ,\n",
       "       0.        , 0.        , 1.0086564 , 0.        , 1.0113579 ,\n",
       "       0.        , 0.9963773 , 0.        , 0.98171645, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.993469  , 0.9994917 ,\n",
       "       0.        , 1.0332388 , 1.022891  , 1.0120986 , 0.9996393 ,\n",
       "       0.98732764, 0.        , 0.9854657 , 0.990705  , 0.99340606,\n",
       "       0.        , 0.988576  , 0.9898397 , 0.9760362 , 1.0164539 ,\n",
       "       0.        , 0.        , 0.9727209 , 0.        , 0.9799903 ,\n",
       "       1.0049362 , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_np_pred_rate_val[ np_accuracy[:]==False ] = 0\n",
    "b_np_pred_rate_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ad710d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_rate_val[ np_accuracy[:]==1 ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6532fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n"
     ]
    }
   ],
   "source": [
    "print(992 - np.count_nonzero(np_pred_rate_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078d868a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for i in np_pred_rate_val:\n",
    "    if i == 0:\n",
    "        c += 1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533a5b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4984894259818731"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c/993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6f571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
