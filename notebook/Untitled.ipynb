{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62795672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Data_preprocess():\n",
    "    def __init__(self) :\n",
    "        return\n",
    "        \n",
    "    def abc(self) :\n",
    "        return 1, 2, 3\n",
    "    \n",
    "    def der(self, b) :\n",
    "        return b\n",
    "ba = 1234\n",
    "abcd = Data_preprocess()\n",
    "\n",
    "c = abcd.der(b=ba)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23ed6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyupbit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "class Data_preprocess():\n",
    "    def __init__(self, ticker, interval, to, count):\n",
    "        self.data, self.label, self.dataset = self.preprocess(\n",
    "            pyupbit.get_ohlcv(ticker=ticker, interval=interval, to=to, count=count))\n",
    "\n",
    "    def MinMax(self, dataset_df):\n",
    "        norm = MinMaxScaler()\n",
    "        norm_dataset = norm.fit_transform(dataset_df)\n",
    "        return pd.DataFrame(norm_dataset, columns=list(dataset_df.columns))\n",
    "\n",
    "    def add_after10(self, dataset_df):\n",
    "        after10 = np.zeros_like(self.norm_dataset['close'])\n",
    "        for i in range(len(dataset_df['close']) - 1):\n",
    "            after10[i] = dataset_df['close'][i + 1]\n",
    "        return after10\n",
    "\n",
    "    def drop_feature(self, dataset_df):\n",
    "        # index(시간) 제거\n",
    "        dataset_df = dataset_df.reset_index(drop=True)\n",
    "        # value 제거\n",
    "        dataset_df = dataset_df.drop(columns=['value'])\n",
    "        return dataset_df\n",
    "\n",
    "    def add_avgPrice(self, dataset_df):\n",
    "        return (dataset_df['high'] + dataset_df['low'] +\n",
    "                dataset_df['open'] + dataset_df['close']) // 4\n",
    "\n",
    "    def preprocess(self, dataset, latest=False):\n",
    "\n",
    "        # drop feature\n",
    "        dataset_df = self.drop_feature(dataset)\n",
    "\n",
    "        # avg_price 추가\n",
    "        dataset_df['avg_price'] = self.add_avgPrice(dataset_df)\n",
    "\n",
    "        if latest == True:\n",
    "            # 가장 예전 데이터 삭제 - norm이랑 original 둘 다 적용\n",
    "            self.dataset = self.dataset.drop([self.dataset.index[0]]).drop(columns=['after10'])\n",
    "            self.norm_dataset = self.norm_dataset.drop([self.norm_dataset.index[0]])\n",
    "\n",
    "            # ori dataset에 추가\n",
    "            self.dataset = pd.concat([self.dataset, dataset_df])\n",
    "            self.dataset = self.dataset.reset_index(drop=True)\n",
    "\n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(self.dataset)\n",
    "\n",
    "            # after10 추가\n",
    "            self.dataset['after10'] = self.add_after10(self.dataset)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(dataset_df)\n",
    "\n",
    "            # after10 추가\n",
    "            dataset_df['after10'] = self.add_after10(dataset_df)\n",
    "\n",
    "        # 예측될 값(label)인 10분 후 가격\n",
    "        self.norm_dataset['after10'] = self.add_after10(self.norm_dataset)\n",
    "\n",
    "        return self.norm_dataset.drop(columns=['after10']), self.norm_dataset['after10'], dataset_df\n",
    "\n",
    "    # dataset에 window 적용\n",
    "    def windowed_dataset(self, data, label, window_size, batch_size):\n",
    "        sliced_data = tf.data.Dataset.from_tensor_slices(data)\n",
    "        sliced_data = sliced_data.window(window_size, shift=1, stride=1, drop_remainder=True)\n",
    "        sliced_data = sliced_data.flat_map(lambda x: x.batch(window_size))\n",
    "\n",
    "        sliced_label = tf.data.Dataset.from_tensor_slices(label[window_size:])\n",
    "\n",
    "        sliced_dataset = tf.data.Dataset.zip((sliced_data, sliced_label))\n",
    "\n",
    "        return sliced_dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "525ac2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Lambda, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "class Custom_Model():\n",
    "    def __init__(self, input_shape, args):\n",
    "        self.args = args\n",
    "        self.model = self.build_model(input_shape)\n",
    "\n",
    "\n",
    "    def build_model(self, input_shape: tuple):\n",
    "        input = Input(shape=input_shape)\n",
    "        x = LSTM(128, return_sequences=True, activation='tanh', dropout=0.2)(input)\n",
    "        x = LSTM(64, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "        x = LSTM(32, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        output = Dense(1)(x)\n",
    "        return Model(input, output)\n",
    "\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        loss = Huber()\n",
    "        optimizer = Adam(learning_rate=self.args.lr)\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['mse'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def callback(self):\n",
    "        saveCheckpoint = ModelCheckpoint(os.path.join(self.args.save_path, 'checkpoint.ckpt'),\n",
    "                                    save_weights_only=False,\n",
    "                                    save_best_only=True,\n",
    "                                    monitor='val_loss',\n",
    "                                    verbose=1)\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                                      patience=self.args.early_stop)\n",
    "        return [saveCheckpoint, earlyStopping]\n",
    "\n",
    "\n",
    "    def load_model(self, weight):\n",
    "        self.model.load_weights(weight)\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efdb420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 6, 6)]            0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 6, 128)            69120     \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 6, 64)             49408     \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 6, 32)             12416     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 130,977\n",
      "Trainable params: 130,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict \n",
    "\n",
    "    \n",
    "option = {\n",
    "    'save_path' : './checkpoints',\n",
    "    'val_size' : 0.1,\n",
    "    'lr' : 0.0001,\n",
    "    'epoch' : 150,\n",
    "    'batch' : 32,\n",
    "    'early_stop' : 10,\n",
    "    'weight' : None\n",
    "}\n",
    "\n",
    "args = EasyDict(option)\n",
    "WINDOW_SIZE = 6\n",
    "FEATURES = 6\n",
    "\n",
    "#init_model = models.Custom_Model((WINDOW_SIZE,FEATURES), args)\n",
    "init_model = Custom_Model((WINDOW_SIZE,FEATURES), args)\n",
    "model = init_model.model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8206fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model.compile_model(model)\n",
    "\n",
    "callbacks = init_model.callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0fe448f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (10000, 6)\n",
      "label shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "ticker = 'KRW-BTC'\n",
    "interval = 'minute10'\n",
    "to = f'2021-11-10 00:10'\n",
    "count = 10000\n",
    "\n",
    "#processed_data =  dataset.Data_preprocess(ticker, interval, to, count)\n",
    "processed_data =  Data_preprocess(ticker, interval, to, count)\n",
    "\n",
    "print(\"data shape\", processed_data.data.shape)\n",
    "print(\"label shape\", processed_data.label.shape)\n",
    "\n",
    "train_data, val_data, train_label, val_label = train_test_split(\n",
    "    processed_data.data,\n",
    "    processed_data.label,\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=False)\n",
    "\n",
    "train_dataset = processed_data.windowed_dataset(train_data, train_label ,WINDOW_SIZE, args.batch)\n",
    "validation_dataset = processed_data.windowed_dataset(val_data, val_label ,WINDOW_SIZE, args.batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "daf39a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "282/282 [==============================] - 9s 19ms/step - loss: 0.0050 - mse: 0.0100 - val_loss: 0.0017 - val_mse: 0.0035\n",
      "Epoch 2/150\n",
      "282/282 [==============================] - 5s 17ms/step - loss: 0.0016 - mse: 0.0032 - val_loss: 0.0023 - val_mse: 0.0045\n",
      "Epoch 3/150\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.0015 - mse: 0.0030 - val_loss: 0.0031 - val_mse: 0.0062\n",
      "Epoch 4/150\n",
      "282/282 [==============================] - 5s 18ms/step - loss: 0.0014 - mse: 0.0028 - val_loss: 0.0038 - val_mse: 0.0076\n",
      "Epoch 5/150\n",
      "282/282 [==============================] - 5s 17ms/step - loss: 0.0012 - mse: 0.0025 - val_loss: 0.0036 - val_mse: 0.0071\n",
      "Epoch 6/150\n",
      "282/282 [==============================] - 4s 14ms/step - loss: 0.0013 - mse: 0.0025 - val_loss: 0.0045 - val_mse: 0.0090\n",
      "Epoch 7/150\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.0012 - mse: 0.0024 - val_loss: 0.0043 - val_mse: 0.0087\n",
      "Epoch 8/150\n",
      "282/282 [==============================] - 4s 15ms/step - loss: 0.0012 - mse: 0.0024 - val_loss: 0.0044 - val_mse: 0.0088\n",
      "Epoch 9/150\n",
      "282/282 [==============================] - 4s 15ms/step - loss: 0.0012 - mse: 0.0024 - val_loss: 0.0048 - val_mse: 0.0097\n",
      "Epoch 10/150\n",
      "100/282 [=========>....................] - ETA: 3s - loss: 0.0011 - mse: 0.0022- ETA: 3s - loss: 0.0015 -"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-5a6d834b97e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[1;31m#callbacks=callbacks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-5a6d834b97e9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, callbacks)\u001b[0m\n\u001b[0;32m      2\u001b[0m     hisotry = model.fit(train_data,\n\u001b[0;32m      3\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                 epochs=150)\n\u001b[0m\u001b[0;32m      5\u001b[0m                 \u001b[1;31m#callbacks=callbacks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\coin\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\coin\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, train_data, val_data, callbacks) :\n",
    "    hisotry = model.fit(train_data,\n",
    "                validation_data=(val_data),\n",
    "                epochs=150)\n",
    "                #callbacks=callbacks)\n",
    "    ret\n",
    "train(model, train_dataset, validation_dataset, callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af407",
   "metadata": {},
   "source": [
    "asdf  \n",
    "┣??  \n",
    "┖??\n",
    "\n",
    "??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d4fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
