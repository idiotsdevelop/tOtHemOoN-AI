{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba110fb",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4505d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyupbit\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0d3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocess() :\n",
    "    def __init__(self, ticker, interval, to, count) :\n",
    "        self.data, self.label, self.dataset = self.preprocess(pyupbit.get_ohlcv(ticker=ticker, interval=interval, to=to, count=count))\n",
    "    \n",
    "    def MinMax(self, dataset_df) :\n",
    "        norm = MinMaxScaler()\n",
    "        norm_dataset = norm.fit_transform(dataset_df)\n",
    "        return pd.DataFrame(norm_dataset, columns=list(dataset_df.columns))\n",
    "    \n",
    "    \n",
    "#     def add_after10(self, dataset_df) :\n",
    "#         after10 = np.zeros_like(self.norm_dataset['close'])\n",
    "#         for i in range(len(dataset_df['close']) - 1) :\n",
    "#             after10[i] = dataset_df['close'][i + 1]\n",
    "#         return after10\n",
    "    def add_after10(self, dataset_df) :\n",
    "        after10 = np.zeros_like(self.norm_dataset['close'])\n",
    "        for i in range(len(dataset_df['close']) - 1) :\n",
    "            if dataset_df['close'][i + 1] > dataset_df['close'][i] :\n",
    "                after10[i] = 1\n",
    "            else : \n",
    "                after10[i] = 0\n",
    "            \n",
    "        return after10\n",
    "    \n",
    "    \n",
    "    def drop_feature(self, dataset_df) :\n",
    "        # index(시간) 제거\n",
    "        dataset_df = dataset_df.reset_index(drop=True)\n",
    "        # value 제거\n",
    "        dataset_df = dataset_df.drop(columns=['value'])\n",
    "        return dataset_df\n",
    "    \n",
    "    \n",
    "    def add_avgPrice(self, dataset_df) :\n",
    "        return (dataset_df['high'] + dataset_df['low'] + \n",
    "                dataset_df['open'] + dataset_df['close']) // 4\n",
    "       \n",
    "    \n",
    "    def preprocess(self, dataset, latest=False) :\n",
    "        \n",
    "        # drop feature\n",
    "        dataset_df = self.drop_feature(dataset)\n",
    "        \n",
    "        # avg_price 추가\n",
    "        dataset_df['avg_price'] = self.add_avgPrice(dataset_df)\n",
    "        \n",
    "        if latest == True :\n",
    "            # 가장 예전 데이터 삭제 - norm이랑 original 둘 다 적용\n",
    "            self.dataset = self.dataset.drop([self.dataset.index[0]]).drop(columns=['after10'])\n",
    "            self.norm_dataset = self.norm_dataset.drop([self.norm_dataset.index[0]])\n",
    "\n",
    "            # ori dataset에 추가\n",
    "            self.dataset = pd.concat([self.dataset, dataset_df])\n",
    "            self.dataset = self.dataset.reset_index(drop=True)\n",
    "            \n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(self.dataset)\n",
    "            \n",
    "            # after10 추가\n",
    "            self.dataset['after10'] = self.add_after10(self.dataset)\n",
    "            \n",
    "        \n",
    "        else :\n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(dataset_df)\n",
    "            \n",
    "            # after10 추가\n",
    "            dataset_df['after10'] = self.add_after10(dataset_df)\n",
    "        \n",
    "        # 예측될 값(label)인 10분 후 가격\n",
    "        self.norm_dataset['after10'] = self.add_after10(self.norm_dataset)\n",
    "        \n",
    "        # 마지막 행 삭제 (다음 가격을 모르므로 라벨을 붙일 수 없음)\n",
    "        self.norm_dataset = self.norm_dataset.iloc[:-1]\n",
    "        \n",
    "        return self.norm_dataset.drop(columns=['after10']), self.norm_dataset['after10'], dataset_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c97350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>avg_price</th>\n",
       "      <th>after10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75025000.0</td>\n",
       "      <td>75110000.0</td>\n",
       "      <td>74808000.0</td>\n",
       "      <td>74827000.0</td>\n",
       "      <td>42.323616</td>\n",
       "      <td>74942500.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74827000.0</td>\n",
       "      <td>74955000.0</td>\n",
       "      <td>74810000.0</td>\n",
       "      <td>74886000.0</td>\n",
       "      <td>26.774901</td>\n",
       "      <td>74869500.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74886000.0</td>\n",
       "      <td>75023000.0</td>\n",
       "      <td>74885000.0</td>\n",
       "      <td>74913000.0</td>\n",
       "      <td>27.888634</td>\n",
       "      <td>74926750.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74914000.0</td>\n",
       "      <td>75019000.0</td>\n",
       "      <td>74880000.0</td>\n",
       "      <td>74982000.0</td>\n",
       "      <td>29.500190</td>\n",
       "      <td>74948750.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74981000.0</td>\n",
       "      <td>74981000.0</td>\n",
       "      <td>74699000.0</td>\n",
       "      <td>74733000.0</td>\n",
       "      <td>24.260177</td>\n",
       "      <td>74848500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>81359000.0</td>\n",
       "      <td>81359000.0</td>\n",
       "      <td>81166000.0</td>\n",
       "      <td>81203000.0</td>\n",
       "      <td>54.985089</td>\n",
       "      <td>81271750.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>81203000.0</td>\n",
       "      <td>81265000.0</td>\n",
       "      <td>81080000.0</td>\n",
       "      <td>81118000.0</td>\n",
       "      <td>46.954699</td>\n",
       "      <td>81166500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>81118000.0</td>\n",
       "      <td>81385000.0</td>\n",
       "      <td>80756000.0</td>\n",
       "      <td>80776000.0</td>\n",
       "      <td>60.260760</td>\n",
       "      <td>81008750.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>80776000.0</td>\n",
       "      <td>80999000.0</td>\n",
       "      <td>80550000.0</td>\n",
       "      <td>80650000.0</td>\n",
       "      <td>99.310089</td>\n",
       "      <td>80743750.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>80650000.0</td>\n",
       "      <td>80688000.0</td>\n",
       "      <td>80277000.0</td>\n",
       "      <td>80441000.0</td>\n",
       "      <td>167.911427</td>\n",
       "      <td>80514000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           open        high         low       close      volume   avg_price  \\\n",
       "0    75025000.0  75110000.0  74808000.0  74827000.0   42.323616  74942500.0   \n",
       "1    74827000.0  74955000.0  74810000.0  74886000.0   26.774901  74869500.0   \n",
       "2    74886000.0  75023000.0  74885000.0  74913000.0   27.888634  74926750.0   \n",
       "3    74914000.0  75019000.0  74880000.0  74982000.0   29.500190  74948750.0   \n",
       "4    74981000.0  74981000.0  74699000.0  74733000.0   24.260177  74848500.0   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "995  81359000.0  81359000.0  81166000.0  81203000.0   54.985089  81271750.0   \n",
       "996  81203000.0  81265000.0  81080000.0  81118000.0   46.954699  81166500.0   \n",
       "997  81118000.0  81385000.0  80756000.0  80776000.0   60.260760  81008750.0   \n",
       "998  80776000.0  80999000.0  80550000.0  80650000.0   99.310089  80743750.0   \n",
       "999  80650000.0  80688000.0  80277000.0  80441000.0  167.911427  80514000.0   \n",
       "\n",
       "     after10  \n",
       "0        1.0  \n",
       "1        1.0  \n",
       "2        1.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "..       ...  \n",
       "995      0.0  \n",
       "996      0.0  \n",
       "997      0.0  \n",
       "998      0.0  \n",
       "999      0.0  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>avg_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.224190</td>\n",
       "      <td>0.218171</td>\n",
       "      <td>0.236572</td>\n",
       "      <td>0.203964</td>\n",
       "      <td>0.122389</td>\n",
       "      <td>0.214193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204150</td>\n",
       "      <td>0.202204</td>\n",
       "      <td>0.236777</td>\n",
       "      <td>0.209930</td>\n",
       "      <td>0.074569</td>\n",
       "      <td>0.206586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.210121</td>\n",
       "      <td>0.209209</td>\n",
       "      <td>0.244465</td>\n",
       "      <td>0.212661</td>\n",
       "      <td>0.077994</td>\n",
       "      <td>0.212551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.212955</td>\n",
       "      <td>0.208797</td>\n",
       "      <td>0.243952</td>\n",
       "      <td>0.219638</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.214844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.219737</td>\n",
       "      <td>0.204883</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.194458</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>0.204397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.873918</td>\n",
       "      <td>0.893706</td>\n",
       "      <td>0.864496</td>\n",
       "      <td>0.155024</td>\n",
       "      <td>0.880425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.865283</td>\n",
       "      <td>0.861867</td>\n",
       "      <td>0.888274</td>\n",
       "      <td>0.848721</td>\n",
       "      <td>0.161329</td>\n",
       "      <td>0.873730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.849494</td>\n",
       "      <td>0.852184</td>\n",
       "      <td>0.879459</td>\n",
       "      <td>0.840125</td>\n",
       "      <td>0.136632</td>\n",
       "      <td>0.862762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.840891</td>\n",
       "      <td>0.864545</td>\n",
       "      <td>0.846248</td>\n",
       "      <td>0.805542</td>\n",
       "      <td>0.177554</td>\n",
       "      <td>0.846324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.806275</td>\n",
       "      <td>0.824784</td>\n",
       "      <td>0.825133</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>0.297649</td>\n",
       "      <td>0.818710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         open      high       low     close    volume  avg_price\n",
       "0    0.224190  0.218171  0.236572  0.203964  0.122389   0.214193\n",
       "1    0.204150  0.202204  0.236777  0.209930  0.074569   0.206586\n",
       "2    0.210121  0.209209  0.244465  0.212661  0.077994   0.212551\n",
       "3    0.212955  0.208797  0.243952  0.219638  0.082951   0.214844\n",
       "4    0.219737  0.204883  0.225400  0.194458  0.066835   0.204397\n",
       "..        ...       ...       ...       ...       ...        ...\n",
       "994  0.858300  0.873918  0.893706  0.864496  0.155024   0.880425\n",
       "995  0.865283  0.861867  0.888274  0.848721  0.161329   0.873730\n",
       "996  0.849494  0.852184  0.879459  0.840125  0.136632   0.862762\n",
       "997  0.840891  0.864545  0.846248  0.805542  0.177554   0.846324\n",
       "998  0.806275  0.824784  0.825133  0.792800  0.297649   0.818710\n",
       "\n",
       "[999 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      0.0\n",
       "4      0.0\n",
       "      ... \n",
       "994    0.0\n",
       "995    0.0\n",
       "996    0.0\n",
       "997    0.0\n",
       "998    0.0\n",
       "Name: after10, Length: 999, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ticker = 'KRW-BTC'\n",
    "interval ='minute10'\n",
    "to = f'2021-11-10 00:10'\n",
    "count = 1000\n",
    "\n",
    "processed_data =  Data_preprocess(ticker, interval, to, count)\n",
    "display(processed_data.dataset)\n",
    "display(processed_data.data)\n",
    "display(processed_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd671a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(999, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(999,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(processed_data.data))\n",
    "display(len(processed_data.label))\n",
    "\n",
    "display((processed_data.data).shape)\n",
    "display((processed_data.label).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593aa5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([999, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2242, 0.2182, 0.2366, 0.2040, 0.1224, 0.2142],\n",
      "        [0.2041, 0.2022, 0.2368, 0.2099, 0.0746, 0.2066],\n",
      "        [0.2101, 0.2092, 0.2445, 0.2127, 0.0780, 0.2126],\n",
      "        ...,\n",
      "        [0.8495, 0.8522, 0.8795, 0.8401, 0.1366, 0.8628],\n",
      "        [0.8409, 0.8645, 0.8462, 0.8055, 0.1776, 0.8463],\n",
      "        [0.8063, 0.8248, 0.8251, 0.7928, 0.2976, 0.8187]])\n"
     ]
    }
   ],
   "source": [
    "display((processed_data.data).shape[0])\n",
    "tensor_data = torch.Tensor((processed_data.data).values)\n",
    "display(tensor_data.shape)\n",
    "print(tensor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fbd4f",
   "metadata": {},
   "source": [
    "https://doheon.github.io/%EC%BD%94%EB%93%9C%EA%B5%AC%ED%98%84/time-series/ci-4.transformer-post/\n",
    "\n",
    "참고해서 WindowDataset 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232d04e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape ====  (80, 600, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 600, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def WindowDataGenerator(df_data, df_label, window_size, stride) :\n",
    "    num_sample = (df_data.shape[0] - window_size) // stride + 1     \n",
    "\n",
    "    data = np.zeros([window_size, df_data.shape[1], num_sample])\n",
    "    labels = np.zeros([num_sample])\n",
    "\n",
    "    for i in range(num_sample) :\n",
    "        data_start = stride * i\n",
    "        data_end = data_start + window_size\n",
    "        data[:, :, i] = df_data[data_start : data_end, :]\n",
    "        labels[i] = df_label[data_end - 1]\n",
    "\n",
    "    data = data.transpose((2, 0, 1))\n",
    "    print(\"dataset shape ==== \",data.shape)\n",
    "    # data shape (80, 600, 6), label shape (80,)\n",
    "    return torch.Tensor(data), torch.Tensor(labels)\n",
    "\n",
    "a,b = WindowDataGenerator(processed_data.data.values, processed_data.label.values, 600, 5)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7e060",
   "metadata": {},
   "source": [
    "# Window Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23edfc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDataset(Dataset) :\n",
    "    def __init__(self, df_data, df_label, window_size=600, stride=5) :\n",
    "        self.data, self.label = self.WindowDataGenerator(df_data, df_label, window_size, stride)        \n",
    "        \n",
    "    def __getitem__(self, i) :\n",
    "        return self.data[i], self.label[i]\n",
    "                \n",
    "    def __len__(self) :\n",
    "        assert len(self.data) == len(self.label), \"data와 label의 길이가 다름\"\n",
    "        return len(self.data)\n",
    "        \n",
    "    \n",
    "    def WindowDataGenerator(self, df_data, df_label, window_size, stride) :\n",
    "        num_sample = (df_data.shape[0] - window_size) // stride + 1     \n",
    "\n",
    "        data = np.zeros([window_size, df_data.shape[1], num_sample])\n",
    "        labels = np.zeros([num_sample])\n",
    "\n",
    "        for i in range(num_sample) :\n",
    "            data_start = stride * i\n",
    "            data_end = data_start + window_size\n",
    "            data[:, :, i] = df_data[data_start : data_end, :]\n",
    "            labels[i] = df_label[data_end - 1]\n",
    "\n",
    "        data = data.transpose((2, 0, 1))\n",
    "        # data shape (80, 600, 6), label shape (80,)\n",
    "        return torch.Tensor(data), torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2de4f",
   "metadata": {},
   "source": [
    "# nn.Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0980eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer2FC(nn.Module) :\n",
    "    def __init__(self, input_shape, d_model, n_head, num_layer, dropout, num_class=2):\n",
    "        super(Transformer2FC, self).__init__()\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layer)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape[1], d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, d_model)\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, 1)\n",
    "        )\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], input_shape[0]//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_shape[0]//2, num_class)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x, masked_x) :\n",
    "        # (batch, data, dim)\n",
    "        x = self.Encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x.transpose(0,1), masked_x).transpose(0, 1)\n",
    "        x = self.linear(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = self.linear2(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module) :\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000) :\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "def get_attention_mask(x) :\n",
    "    mask = torch.eq(x, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7a6a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "lr = 1e-4\n",
    "epoch = 1\n",
    "window_size = 600\n",
    "feature_len = 6\n",
    "model = Transformer2FC((window_size, feature_len), 512, 8, 4, 0.1, num_class=1).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7864db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WindowDataset(processed_data.data.values, \n",
    "                        processed_data.label.values, \n",
    "                       window_size = 600, \n",
    "                       stride = 5)\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07b7b55c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7236, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.9817, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.8672, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6994, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7396, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6867, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6960, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6957, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7306, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6602, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6566, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6962, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.8185, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7790, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7415, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7094, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6983, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6926, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6950, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5762, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5331, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-77019e27a3d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msrc_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-3ba9131e84d2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, masked_x)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasked_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    348\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                            need_weights=False)[0]\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[0;32m   1008\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m                 attn_mask=attn_mask)\n\u001b[0m\u001b[0;32m   1011\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[0;32m   5099\u001b[0m     \u001b[1;31m# (deep breath) calculate attention and out projection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5100\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5101\u001b[1;33m     \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5102\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5103\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[0;32m   4845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4846\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4847\u001b[1;33m     \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4848\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4849\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\coin-torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "progress = tqdm(range(epoch))\n",
    "\n",
    "model.train()\n",
    "for i in progress :\n",
    "    batchloss = 0.0\n",
    "    \n",
    "    for (data, label) in dataloader :\n",
    "        optim.zero_grad()\n",
    "        src_mask = model.generate_square_subsequent_mask(data.shape[1]).to(device)\n",
    "        pred = model(data.float().to(device), src_mask)\n",
    "        loss = criterion(pred, label.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        batchloss += loss\n",
    "        print(loss, end=\"\\n\")\n",
    "    progress.set_decription(\"loss : {:0.6f}\".format(batchloss.cpu().item() / len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 5)\n",
    "x = x.transpose(0,1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97cdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0ece4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
