{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78ba7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyupbit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "class Data_preprocess():\n",
    "    def __init__(self, ticker, interval, to, count):\n",
    "        self.data, self.label, self.dataset = self.preprocess(\n",
    "            pyupbit.get_ohlcv(ticker=ticker, interval=interval, to=to, count=count))\n",
    "\n",
    "    def MinMax(self, dataset_df):\n",
    "        norm = MinMaxScaler()\n",
    "        norm_dataset = norm.fit_transform(dataset_df)\n",
    "        return pd.DataFrame(norm_dataset, columns=list(dataset_df.columns))\n",
    "\n",
    "    def add_after10(self, dataset_df):\n",
    "        after10 = np.zeros_like(self.norm_dataset['close'])\n",
    "        for i in range(len(dataset_df['close']) - 1):\n",
    "            after10[i] = dataset_df['close'][i + 1]\n",
    "        return after10\n",
    "\n",
    "    def drop_feature(self, dataset_df):\n",
    "        # index(시간) 제거\n",
    "        dataset_df = dataset_df.reset_index(drop=True)\n",
    "        # value 제거\n",
    "        dataset_df = dataset_df.drop(columns=['value'])\n",
    "        return dataset_df\n",
    "\n",
    "    def add_avgPrice(self, dataset_df):\n",
    "        return (dataset_df['high'] + dataset_df['low'] +\n",
    "                dataset_df['open'] + dataset_df['close']) // 4\n",
    "\n",
    "    def preprocess(self, dataset, latest=False):\n",
    "\n",
    "        # drop feature\n",
    "        dataset_df = self.drop_feature(dataset)\n",
    "\n",
    "        # avg_price 추가\n",
    "        dataset_df['avg_price'] = self.add_avgPrice(dataset_df)\n",
    "\n",
    "        if latest == True:\n",
    "            # 가장 예전 데이터 삭제 - norm이랑 original 둘 다 적용\n",
    "            self.dataset = self.dataset.drop([self.dataset.index[0]]).drop(columns=['after10'])\n",
    "            self.norm_dataset = self.norm_dataset.drop([self.norm_dataset.index[0]])\n",
    "\n",
    "            # ori dataset에 추가\n",
    "            self.dataset = pd.concat([self.dataset, dataset_df])\n",
    "            self.dataset = self.dataset.reset_index(drop=True)\n",
    "\n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(self.dataset)\n",
    "\n",
    "            # after10 추가\n",
    "            self.dataset['after10'] = self.add_after10(self.dataset)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # min max 정규화 (MinMaxScaler) 적용\n",
    "            self.norm_dataset = self.MinMax(dataset_df)\n",
    "\n",
    "            # after10 추가\n",
    "            dataset_df['after10'] = self.add_after10(dataset_df)\n",
    "\n",
    "        # 예측될 값(label)인 10분 후 가격\n",
    "        self.norm_dataset['after10'] = self.add_after10(self.norm_dataset)\n",
    "\n",
    "        return self.norm_dataset.drop(columns=['after10']), self.norm_dataset['after10'], dataset_df\n",
    "\n",
    "    # dataset에 window 적용\n",
    "    def windowed_dataset(self, data, label, window_size, batch_size):\n",
    "        sliced_data = tf.data.Dataset.from_tensor_slices(data)\n",
    "        sliced_data = sliced_data.window(window_size, shift=1, stride=1, drop_remainder=True)\n",
    "        sliced_data = sliced_data.flat_map(lambda x: x.batch(window_size))\n",
    "\n",
    "        sliced_label = tf.data.Dataset.from_tensor_slices(label[window_size:])\n",
    "\n",
    "        sliced_dataset = tf.data.Dataset.zip((sliced_data, sliced_label))\n",
    "\n",
    "        return sliced_dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74bbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Lambda, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "class Custom_Model():\n",
    "    def __init__(self, input_shape, args):\n",
    "        self.args = args\n",
    "        self.model = self.build_model(input_shape)\n",
    "\n",
    "\n",
    "    def build_model(self, input_shape: tuple):\n",
    "        input = Input(shape=input_shape)\n",
    "        x = LSTM(128, return_sequences=True, activation='tanh', dropout=0.2)(input)\n",
    "        x = LSTM(64, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "        x = LSTM(32, return_sequences=True, activation='tanh', dropout=0.2)(x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        output = Dense(1)(x)\n",
    "        return Model(input, output)\n",
    "\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        loss = Huber()\n",
    "        optimizer = Adam(lr=self.args.lr)\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['mse'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def callback(self):\n",
    "        saveCheckpoint = ModelCheckpoint(os.path.join(self.args.save_path, 'checkpoint.ckpt'),\n",
    "                                    save_weights_only=False,\n",
    "                                    save_best_only=True,\n",
    "                                    monitor='val_loss',\n",
    "                                    verbose=1)\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                                      patience=self.args.early_stop)\n",
    "        return [saveCheckpoint, earlyStopping]\n",
    "\n",
    "\n",
    "    def load_model(self, weight):\n",
    "        self.model.load_weights(weight)\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adec395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WINDOW_SIZE = 6\n",
    "FEATURES = 6\n",
    "\n",
    "init_model = models.Custom_Model((WINDOW_SIZE,FEATURES), args)\n",
    "model = init_model.model\n",
    "model = init_model.compile_model(model)\n",
    "\n",
    "callbacks = init_model.callback()\n",
    "\n",
    "ticker = 'KRW-BTC'\n",
    "interval = 'minute10'\n",
    "to = f'2021-11-10 00:10'\n",
    "count = 1000\n",
    "\n",
    "processed_data =  dataset.Data_preprocess(ticker, interval, to, count)\n",
    "\n",
    "train_data, train_label, val_data, val_label = train_test_split(\n",
    "    processed_data.data,\n",
    "    processed_data.label,\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=False)\n",
    "\n",
    "train_dataset = processed_data.windowed_dataset(train_data, train_label ,WINDOW_SIZE, FEATURES)\n",
    "validation_dataset = processed_data.windowed_dataset(val_data, val_label ,WINDOW_SIZE, FEATURES)\n",
    "\n",
    "\n",
    "train(model, train_dataset, validation_dataset, callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
